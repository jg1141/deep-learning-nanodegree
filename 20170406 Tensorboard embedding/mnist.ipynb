{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2017 Google, Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "#scalars tab is for scalar summaries\n",
    "#accuracy and cost function\n",
    "#x axis shows time steps\n",
    "#y axis shows accuracy or loss\n",
    "#increase graph for closer look or view wider range of data points (expands y axis) with these two buttons \n",
    "#can also draw rectange to zoom in on region\n",
    "#double click to zoom out\n",
    "#as we mouse over chart, it will produce crosshairs with data values\n",
    "#we can adjust smoothness of line with slider\n",
    "#step option shows timesteps\n",
    "#relative option shows time relative to first data point (num minutes or hours since training run was started)\n",
    "#wall shows actual times runs happened\n",
    "#we can create new tags in side bar, entirely new or can group existing summaries together\n",
    "\n",
    "\n",
    "#images tab\n",
    "#image summary \n",
    "\n",
    "#still haven't seen audio example in TF \n",
    "#magenta is a good example \n",
    "\n",
    "#graph tab\n",
    "#name scopes as squares\n",
    "#direction of arrows shows the direction tensors are flowing\n",
    "#placeholder x shows entrypoint for data \n",
    "#to reduce clutter TB shows node connected to many others nodes in there own section in detail\n",
    "#we could add and remove to main graph if we wanted to\n",
    "#we could see the values at each time step if we wanted to \n",
    "#we can see either which structure are being used or which devices each oeration is running on \n",
    "#we could manually upload a saved TF graph right from the UI if we'd like \n",
    "#the graph detects structures that are the exact same and colors them the same , unless its grey thats just default\n",
    "#we can see how many tensors inside the lines\n",
    "#more name scopes, the more abstraction, simpler the graph\n",
    "\n",
    "#distributions\n",
    "#to visualize the distributions of activations coming off a particular layer, or the distribution of gradients or weights.\n",
    "\n",
    "#histogram\n",
    "#The histogram plot allows you to plot variables from your graph.\n",
    "#So if you're model has  weights, the histogram shows how the values of those weights change with training.\n",
    "\n",
    "#embeddings\n",
    "#PCA vs T-SNE\n",
    "#The idea of SNE and t-SNE is to place neighbors close to each other, (almost) completly ignoring the global structure.\n",
    "#PCA is quite the opposite. It tries to preserve the global properties (eigenvectors with high variance) while it may lose low-variance deviations between neighbors.\n",
    "#http://stats.stackexchange.com/questions/238538/are-there-cases-where-pca-is-more-suitable-than-t-sne/249520#249520\n",
    "#can also load training weights\n",
    "#find neighbors\n",
    "#share finding\n",
    "#need moar plugins\n",
    "#You can also construct custom specialized linear projections based on text searches for finding meaningful directions in space. \n",
    "\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#versioning, urllib named differently for dif python versions\n",
    "if sys.version_info[0] >= 3:\n",
    "  from urllib.request import urlretrieve\n",
    "else:\n",
    "  from urllib import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define our github URLs\n",
    "LOGDIR = 'mnist_tutorial/'\n",
    "GITHUB_URL ='https://raw.githubusercontent.com/mamcgrath/TensorBoard-TF-Dev-Summit-Tutorial/master/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting mnist_tutorial/data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting mnist_tutorial/data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist_tutorial/data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist_tutorial/data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mnist_tutorial/sprite_1024.png', <http.client.HTTPMessage at 0x1a275611cc0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MNIST EMBEDDINGS ###\n",
    "#downloads and reads the data\n",
    "#The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation).\n",
    "#very MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\".\n",
    "#Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "#https://www.tensorflow.org/images/MNIST-Matrix.png\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=LOGDIR + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "#If you have images associated with your embeddings, you will need to produce a single image consisting of\n",
    "# small thumbnails of each data point. This is known as the sprite image. The sprite should have the same number \n",
    "#of rows and columns with thumbnails stored in row-first order: the first data point placed in the \n",
    "#top left and the last data point in the bottom right:\n",
    "#TSV is a file extension for a tab-delimited file used with spreadsheet software. \n",
    "#TSV stands for Tab Separated Values. TSV files are used for raw data and can be \n",
    "#imported into and exported from spreadsheet software.\n",
    "urlretrieve(GITHUB_URL + 'labels_1024.tsv', LOGDIR + 'labels_1024.tsv')\n",
    "urlretrieve(GITHUB_URL + 'sprite_1024.png', LOGDIR + 'sprite_1024.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first we'll define layers as re-usable functions\n",
    "\n",
    "#Typically, a CNN is composed of a stack of convolutional modules that perform feature extraction. \n",
    "#Each module consists of a convolutional layer followed by a pooling layer. The last convolutional module \n",
    "#is followed by one or more dense layers that perform classification. The final dense layer in a CNN contains \n",
    "#a single node for each target class in the model (all the possible classes the model may predict), with a \n",
    "#softmax activation function to generate a value between 0â€“1 for each node (the sum of all these softmax values \n",
    "#is equal to 1). We can interpret the softmax values for a given image as relative measurements of how likely \n",
    "#it is that the image falls into each target class.\n",
    "\n",
    "\n",
    "#Summary - tensor flow op that outputs protocol buffers\n",
    "#Scalar summary - single value  (line charts) \n",
    "#Image summary (visualize images) generative\n",
    "#Audio\n",
    "#Histogram (see dsitbrutiuons of values ) weights \n",
    "#Tensor (any kind) in development\n",
    "# we can merge them all at the end\n",
    "\n",
    "# Add convolution layer\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "  #tf.name_scope creates namespace for operators in the default graph, places into group, easier to read\n",
    "  #A graph maintains a stack of name scopes. A `with name_scope(...):`\n",
    "  #statement pushes a new name onto the stack for the lifetime of the context.\n",
    "  #Ops have names, name scopes group ops\n",
    "  with tf.name_scope(name):\n",
    "    #A variable maintains state in the graph across calls to run(). You add a variable to the graph by constructing an instance of the class Variable.\n",
    "    #truncated normal Outputs random values from a truncated normal distribution.\n",
    "    w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "    #constant Creates a constant tensor.\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "    #Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "    #1 Flattens the filter to a 2-D matrix\n",
    "    #2 Extracts image patches from the input tensor to form a virtual tensor\n",
    "    #3 For each patch, right-multiplies the filter matrix and the image patch vector.\n",
    "    conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    #nonlin relu reduces likelihood of vanishing gradient, most used activation function these days\n",
    "    act = tf.nn.relu(conv + b)\n",
    "    #     , or the distribution of gradients or weights. \n",
    "    #we can collect this data by attaching tf.summary.histogram ops to the gradient outputs and to the variable that holds weights, respectively.\n",
    "    #visualize the the distribution of weights and biases\n",
    "    tf.summary.histogram(\"weights\", w)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    ##so we can visualize the distributions of activations coming off this layer\n",
    "    tf.summary.histogram(\"activations\", act)\n",
    "    #Let's say we have an 4x4 matrix representing our initial input. \n",
    "    #Let's say as well that we have a 2x2 filter that we'll run over our input. \n",
    "    #We'll have a stride of 2 (meaning the (dx, dy) for stepping over our input will be (2, 2)) and won't overlap regions.\n",
    "    #For each of the regions represented by the filter, we will take the max of that region and create a new, output matrix \n",
    "    #where each element is the max of a region in the original input.\n",
    "    #https://qph.ec.quoracdn.net/main-qimg-8afedfb2f82f279781bfefa269bc6a90-p\n",
    "    return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dense (fully connected) layers perform classification on the \n",
    "#features extracted by the convolutional layers and downsampled by the \n",
    "#pooling layers. In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "# Add fully connected layer\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "  with tf.name_scope(name):\n",
    "    w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "    #fully connected part\n",
    "    act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "    tf.summary.histogram(\"weights\", w)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"activations\", act)\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build our model\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc, hparam):\n",
    "  tf.reset_default_graph()\n",
    "  sess = tf.Session()\n",
    "  \n",
    "  # Setup placeholders, and reshape the data\n",
    "  #for the data (images)\n",
    "  x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "  x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "  #Outputs a Summary protocol buffer with images.\n",
    "  tf.summary.image('input', x_image, 3)\n",
    "  #for the labels\n",
    "  y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "  #2 conv layers or 1? \n",
    "  if use_two_conv:\n",
    "    conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "    conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "  else:\n",
    "    conv1 = conv_layer(x_image, 1, 64, \"conv\")\n",
    "    conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "  #We can flatten this array into a vector of 28x28 = 784 numbers. \n",
    "  #It doesn't matter how we flatten the array, as long as we're consistent \n",
    "  #between images. From this perspective, the MNIST images are just a bunch of \n",
    "  #points in a 784-dimensional vector space, with a very rich structure \n",
    "  flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "  #2 fully connected layers or one?\n",
    "  if use_two_fc:\n",
    "    #give it the flattened image tensor\n",
    "    fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "    #we want these embeeddings to visualize them later\n",
    "    embedding_input = fc1\n",
    "    embedding_size = 1024\n",
    "    logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "  else:\n",
    "    #else we take them directly from the conv layer\n",
    "    embedding_input = flattened\n",
    "    embedding_size = 7*7*64\n",
    "    #logits the sum of the inputs may not equal 1, that the values are not probabilities\n",
    "    #we'll feed these to the last (softmax) to make them probabilities\n",
    "    logits = fc_layer(flattened, 7*7*64, 10, \"fc\")\n",
    "\n",
    "  #short for cross entropy loss\n",
    "  with tf.name_scope(\"xent\"):\n",
    "    #Computes the mean of elements across dimensions of a tensor.\n",
    "    #so in this case across output probabilties\n",
    "    xent = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=y), name=\"xent\")\n",
    "    #save that single number\n",
    "    tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "  with tf.name_scope(\"train\"):\n",
    "    #Adam offers several advantages over the simple tf.train.GradientDescentOptimizer. \n",
    "    #Foremost is that it uses moving averages of the parameters (momentum); \n",
    "    #This enables Adam to use a larger effective step size, and the algorithm will converge to this step size without fine tuning.\n",
    "    #The main down side of the algorithm is that Adam requires more computation to be performed for each parameter \n",
    "    #in each training step (to maintain the moving averages and variance, and calculate the scaled gradient); \n",
    "    #and more state to be retained for each parameter (approximately tripling the size of the model to store the average and variance for each parameter). \n",
    "    #A simple tf.train.GradientDescentOptimizer could equally be used in your MLP, but would require more hyperparameter tuning before it would converge as quickly.\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "  with tf.name_scope(\"accuracy\"):\n",
    "    #Returns the index with the largest value across axes of a tensor.\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    #Casts a tensor to a new type.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "  #merge them all so one write to disk, more comp efficient\n",
    "  summ = tf.summary.merge_all()\n",
    "\n",
    "  #intiialize embedding matrix as 0s\n",
    "  embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "  #give it calculated embedding\n",
    "  assignment = embedding.assign(embedding_input)\n",
    "  #initialize the saver\n",
    "  # Add ops to save and restore all the variables.\n",
    "  saver = tf.train.Saver()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  #filewriter is how we write the summary protocol buffers to disk\n",
    "  writer = tf.summary.FileWriter(LOGDIR + hparam)\n",
    "  writer.add_graph(sess.graph)\n",
    "\n",
    "  ## Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "  config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "  ## You can add multiple embeddings. Here we add only one.\n",
    "  embedding_config = config.embeddings.add()\n",
    "  embedding_config.tensor_name = embedding.name\n",
    "  embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "  embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "  # Specify the width and height of a single thumbnail.\n",
    "  embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "  tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "  #training step\n",
    "  for i in range(2001):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    if i % 5 == 0:\n",
    "      [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "      writer.add_summary(s, i)\n",
    "    if i % 500 == 0:\n",
    "      sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "      #save checkpoints\n",
    "      saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "  conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "  fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "  return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  # You can try adding some more learning rates\n",
    "  for learning_rate in [1E-4]:\n",
    "\n",
    "    # Include \"False\" as a value to try different model architectures\n",
    "    for use_two_fc in [True]:\n",
    "      for use_two_conv in [True]:\n",
    "        # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2)\n",
    "        hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "        print('Starting run for %s' % hparam)\n",
    "        \n",
    "        # Actually run with the new settings\n",
    "        mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run for lr_1E-04,conv=2,fc=2\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
